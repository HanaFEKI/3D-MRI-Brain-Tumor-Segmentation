{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3992ad07-35e3-4451-9a1f-7aa8481e753c",
   "metadata": {},
   "source": [
    "## 3D MRI Brain Tumor Segmentation with U-Net\n",
    "This script implements a 3D U-Net model for brain tumor segmentation using the BraTS dataset.\n",
    "It includes data preprocessing, model training, and an interactive Gradio interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d5f4c-a6a7-4dd1-b2d7-90572a21e115",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd83dda-433e-4438-bb59-92624139dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\hanof\\anaconda3\\lib\\site-packages (5.19.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (4.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.115.10)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (1.7.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.29.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (2.8.2)\n",
      "Requirement already satisfied: pydub in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.9.9)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.46.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio-client==1.7.2->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from gradio-client==1.7.2->gradio) (15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.20.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hanof\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0900d1ea-b68e-462f-8cf9-07c29db88dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D MRI Brain Tumor Segmentation with U-Net\n",
    "# This script implements a 3D U-Net model for brain tumor segmentation using the BraTS dataset.\n",
    "# It includes data preprocessing, model training, and an interactive Gradio interface.\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import tensorflow as tf\n",
    "import gradio as gr\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import threading\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49226ba5-566d-4a00-82e7-df24eddae35a",
   "metadata": {},
   "source": [
    "### Constants\n",
    "These constants define key parameters for the segmentation task:\n",
    "- `IMG_SIZE = (128, 128, 128)`: Resize all images to 128x128x128 to balance memory usage and detail retention. The BraTS dataset typically has 240x240x155 volumes, but downsizing reduces computational load while preserving sufficient anatomical information.\n",
    "- `NUM_CLASSES = 4`: Represents the four classes in BraTS (0: background, 1: necrosis, 2: edema, 3: enhancing tumor). This matches the dataset's label structure after remapping label 4 to 3.\n",
    "- `CHANNELS = 4`: Corresponds to the four MRI modalities (flair, t1, t1ce, t2) used as input channels, providing multi-modal information for better segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038c7997-fa13-4799-90f5-56cae6f03738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "IMG_SIZE = (128, 128, 128)  # Target size for resizing images\n",
    "NUM_CLASSES = 4  # Number of classes (0: background, 1: necrosis, 2: edema, 3: enhancing tumor)\n",
    "CHANNELS = 4  # Number of MRI modalities (flair, t1, t1ce, t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8613d6b7-3ef6-4c8e-b614-1f86e11c534e",
   "metadata": {},
   "source": [
    "### DatasetHandler Class\n",
    "This class manages loading and preprocessing of MRI data from the BraTS dataset:\n",
    "- **Why MinMaxScaler?**: Normalizes pixel intensities to [0, 1], which stabilizes training by ensuring consistent input ranges across modalities.\n",
    "- **Preprocessing Choices**: Resizing to `IMG_SIZE` ensures uniformity, and one-hot encoding of masks prepares them for categorical cross-entropy loss in the U-Net.\n",
    "- **Multi-Modal Loading**: The `load_sample` method stacks four modalities into a 4D array, leveraging complementary information from each scan type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74723fd3-a1ae-4aae-bd85-0ce1e865fb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "    \"\"\"Handles loading and preprocessing of MRI data.\"\"\"\n",
    "    def __init__(self, dataset_path):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.scaler = MinMaxScaler()  # Normalizes data to [0, 1]\n",
    "    \n",
    "    def load_nifti(self, file_path):\n",
    "        \"\"\"Loads a NIfTI file and returns its data as a numpy array.\"\"\"\n",
    "        try:\n",
    "            return nib.load(file_path).get_fdata()\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load NIfTI file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Normalizes image data to [0, 1] range and resizes.\"\"\"\n",
    "        if image.size == 0:\n",
    "            raise ValueError(\"Empty image provided for preprocessing.\")\n",
    "        image = self.scaler.fit_transform(image.reshape(-1, 1)).reshape(image.shape)\n",
    "        return np.resize(image, IMG_SIZE)  # Resize to consistent dimensions\n",
    "    \n",
    "    def preprocess_mask(self, mask):\n",
    "        \"\"\"Preprocesses segmentation mask: converts to uint8, remaps labels, and one-hot encodes.\"\"\"\n",
    "        mask = mask.astype(np.uint8)\n",
    "        mask[mask == 4] = 3  # Remap label 4 to 3 (BraTS convention)\n",
    "        mask = np.resize(mask, IMG_SIZE)\n",
    "        return to_categorical(mask, num_classes=NUM_CLASSES)  # One-hot encoding for multi-class\n",
    "    \n",
    "    def load_sample(self, sample_dir):\n",
    "        \"\"\"Loads and preprocesses a single BraTS sample with all modalities.\"\"\"\n",
    "        modalities = ['flair', 't1', 't1ce', 't2']\n",
    "        images = []\n",
    "        for modality in modalities:\n",
    "            file_path = f\"{sample_dir}/BraTS20_Training_{sample_dir.split('_')[-1]}_{modality}.nii\"\n",
    "            img = self.load_nifti(file_path)\n",
    "            img = self.preprocess_image(img)\n",
    "            images.append(img)\n",
    "        image_stack = np.stack(images, axis=-1)  # Shape: (128, 128, 128, 4)\n",
    "        \n",
    "        mask_path = f\"{sample_dir}/BraTS20_Training_{sample_dir.split('_')[-1]}_seg.nii\"\n",
    "        mask = self.load_nifti(mask_path)\n",
    "        mask = self.preprocess_mask(mask)\n",
    "        \n",
    "        return image_stack, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320065e8-f2a2-4fd6-9509-a1f86dd51b2b",
   "metadata": {},
   "source": [
    "### UNetModel Class\n",
    "This class manages the 3D U-Net model for brain tumor segmentation:\n",
    "- **Purpose**: Provides a reference implementation of the U-Net architecture and loads a pre-trained model for inference.\n",
    "- **Implementation**: \n",
    "  - **`build_unet` Method**: Defines a 3D U-Net architecture as a reference (not used in this script). It includes an encoder-decoder structure with skip connections:\n",
    "    - **Filter Sizes (32, 64, 128, 256)**: Increase with depth for hierarchical feature extraction, then decrease symmetrically in the decoder.\n",
    "    - **Kernel Size (3,3,3)**: Small kernels reduce parameters while capturing local 3D patterns.\n",
    "    - **Dropout (0.3)**: Added in the bottleneck to prevent overfitting.\n",
    "    - **Pooling/Upsampling (2,2,2)**: Standard resolution adjustment for 3D U-Net.\n",
    "    - **Softmax Output**: Produces probabilities for 4 classes per voxel.\n",
    "  - This implementation is included as an example of how the model could be built if training from scratch, but it is not executed here.\n",
    "- **Pre-trained Model**: The `__init__` method loads `'brats_3d.hdf5'` using `load_model(compile=False)`, which is the actual model used for segmentation in the interactive interface. This pre-trained model overrides the `build_unet` architecture.\n",
    "- **Prediction**: The `predict` method uses the loaded model to generate segmentation masks from input data.\n",
    "- **Why This Structure?**: Including the `build_unet` method provides a clear reference for the U-Net architecture, while loading `'brats_3d.hdf5'` leverages an existing, optimized model for practical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b3f61c61-afb3-4ea2-acda-46bb2ec8962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel:\n",
    "    \"\"\"Loads a pre-trained 3D U-Net model for brain tumor segmentation with reference implementation.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = self.load_pretrained_model()\n",
    "    \n",
    "    def load_pretrained_model(self):\n",
    "        \"\"\"Loads the pre-trained model from 'brats_3d.hdf5'.\"\"\"\n",
    "        try:\n",
    "            return load_model('brats_3d.hdf5', compile=False)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to load pre-trained model 'brats_3d.hdf5': {str(e)}\")\n",
    "    \n",
    "    def build_unet(self):\n",
    "        \"\"\"Reference implementation of the 3D U-Net architecture (not used in inference).\"\"\"\n",
    "        # This is an example of how the U-Net could be implemented if not using a pre-trained model\n",
    "        inputs = Input((*IMG_SIZE, CHANNELS))\n",
    "        \n",
    "        # Encoder\n",
    "        c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "        c1 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c1)\n",
    "        p1 = MaxPooling3D((2, 2, 2))(c1)\n",
    "        \n",
    "        c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(p1)\n",
    "        c2 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c2)\n",
    "        p2 = MaxPooling3D((2, 2, 2))(c2)\n",
    "        \n",
    "        c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(p2)\n",
    "        c3 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(c3)\n",
    "        p3 = MaxPooling3D((2, 2, 2))(c3)\n",
    "        \n",
    "        # Bottleneck\n",
    "        c4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(p3)\n",
    "        c4 = Conv3D(256, (3, 3, 3), activation='relu', padding='same')(c4)\n",
    "        c4 = Dropout(0.3)(c4)\n",
    "        \n",
    "        # Decoder\n",
    "        u5 = UpSampling3D((2, 2, 2))(c4)\n",
    "        u5 = concatenate([u5, c3])\n",
    "        c5 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(u5)\n",
    "        c5 = Conv3D(128, (3, 3, 3), activation='relu', padding='same')(c5)\n",
    "        \n",
    "        u6 = UpSampling3D((2, 2, 2))(c5)\n",
    "        u6 = concatenate([u6, c2])\n",
    "        c6 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(u6)\n",
    "        c6 = Conv3D(64, (3, 3, 3), activation='relu', padding='same')(c6)\n",
    "        \n",
    "        u7 = UpSampling3D((2, 2, 2))(c6)\n",
    "        u7 = concatenate([u7, c1])\n",
    "        c7 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(u7)\n",
    "        c7 = Conv3D(32, (3, 3, 3), activation='relu', padding='same')(c7)\n",
    "        \n",
    "        outputs = Conv3D(NUM_CLASSES, (1, 1, 1), activation='softmax')(c7)\n",
    "        return Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    def predict(self, image):\n",
    "        \"\"\"Predicts segmentation mask for a given image.\"\"\"\n",
    "        return self.model.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c154bf-5410-49e2-acc2-8e3010b27b68",
   "metadata": {},
   "source": [
    "### SegmentationApp Class\n",
    "This class provides the application logic for segmentation:\n",
    "- **Single-File Handling**: For simplicity, it processes a single `.nii` file and simulates 4 channels by duplicating it. In practice, you'd load all modalities.\n",
    "- **Visualization**: Displays a random 2D slice from the 3D prediction using `jet` colormap for clear class distinction.\n",
    "- **Error Handling**: Ensures robustness by catching and reporting issues like file loading failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1373382-62e9-4aad-aa67-2dec4dfec2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationApp:\n",
    "    \"\"\"Application for interactive brain tumor segmentation.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def segment_image(self, npy_data):\n",
    "        \"\"\"Segments a .npy image with 3 channels and returns the result.\"\"\"\n",
    "        try:\n",
    "            test_img_input = np.expand_dims(npy_data, axis=0)  # Add batch dimension\n",
    "            prediction = self.model.predict(test_img_input)\n",
    "            pred_argmax = np.argmax(prediction, axis=4)[0, :, :, :]\n",
    "            \n",
    "            slice_num = random.randint(0, pred_argmax.shape[2] - 1)\n",
    "            \n",
    "            # Display the segmented image\n",
    "            fig_segmented, ax_segmented = plt.subplots(figsize=(8, 8))\n",
    "            ax_segmented.imshow(pred_argmax[:, :, slice_num])\n",
    "            ax_segmented.set_title(\"Segmentation\")\n",
    "            ax_segmented.axis('off')\n",
    "            \n",
    "            return fig_segmented, pred_argmax, npy_data[:, :, slice_num, 0]  # Return plot, prediction, and input slice\n",
    "        except Exception as e:\n",
    "            return None, None, None, f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0acbd5-b26e-435e-bcb6-2308496e0143",
   "metadata": {},
   "source": [
    "### Interactive Segmentation Interface\n",
    "This section defines the Gradio interface integrated with the defined classes:\n",
    "- **Why Gradio?**: Provides an easy-to-use web interface for interactive segmentation, accessible to both technical and non-technical users.\n",
    "- **Training Input**: Uses `.nii` files from the BraTS dataset with 4 modalities (FLAIR, T1, T1ce, T2) processed by `DatasetHandler` for training reference (not executed here as the model is pre-trained).\n",
    "- **Interactive Input**: Accepts a single `.npy` file with 3 channels (FLAIR, T1ce, T2), directly processed without additional preprocessing to match the pre-trained modelâ€™s expected input.\n",
    "- **Output**: Uses `gr.Plot` to display two Matplotlib figures: one for the input image (FLAIR) and one for the segmented result, with a `gr.Textbox` for the diagnosis message.\n",
    "- **Class Integration**: \n",
    "  - `DatasetHandler`: Retained for training reference but not used in inference (direct `.npy` loading as per original logic).\n",
    "  - `UNetModel`: Loads the pre-trained model from `'brats_3d.hdf5'` using `load_model(compile=False)`; includes `build_unet` as a reference implementation (see UNetModel Class).\n",
    "  - `SegmentationApp`: Manages the segmentation process and visualization for the `.npy` input, mirroring the original logic.\n",
    "- **Pre-trained Model**: Loads `'brats_3d.hdf5'` directly. If loading fails, an error message is returned, prompting the user to ensure the model file is present.\n",
    "- **Interactive Control**: Includes a \"Stop Processing\" button to allow users to interrupt the processing and avoid unnecessary computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e18a801a-ea10-4d8a-b92a-b084d1f56416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7882\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7882/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable to stop the processing\n",
    "stop_flag = threading.Event()\n",
    "\n",
    "def interactive_segmentation(npy_file, stop_button):\n",
    "    \"\"\"Interactive segmentation with a 3-channel .npy file using defined classes.\"\"\"\n",
    "    global stop_flag\n",
    "    stop_flag.clear()  # Reset the flag at the start of each execution\n",
    "\n",
    "    if npy_file is None:\n",
    "        return None, None, \"No file uploaded.\"\n",
    "\n",
    "    model = UNetModel()  # Loads 'brats_3d.hdf5'\n",
    "    app = SegmentationApp(model)\n",
    "\n",
    "    try:\n",
    "        # Load the .npy image\n",
    "        test_img = np.load(npy_file.name)\n",
    "        if test_img.ndim != 4 or test_img.shape[-1] != 3:\n",
    "            return None, None, \"Invalid .npy file. It must have shape (H, W, D, 3).\"\n",
    "\n",
    "        # Add a batch dimension for the model\n",
    "        test_img_input = np.expand_dims(test_img, axis=0)\n",
    "\n",
    "        # Make a prediction using the model\n",
    "        test_prediction = model.predict(test_img_input)\n",
    "        test_prediction_argmax = np.argmax(test_prediction, axis=4)[0, :, :, :]\n",
    "\n",
    "        # Select a random slice\n",
    "        n_slice = random.randint(0, test_prediction_argmax.shape[2] - 1)\n",
    "\n",
    "        # Check if the user clicked \"Stop Processing\"\n",
    "        if stop_flag.is_set():\n",
    "            return None, None, \"Processing stopped.\"\n",
    "\n",
    "        # Check if there is a tumor by looking for classes 1 or 3\n",
    "        tumor_found = np.any(np.isin(test_prediction_argmax[:, :, n_slice], [1, 3]))\n",
    "\n",
    "        # Display the input image (FLAIR)\n",
    "        fig_input, ax_input = plt.subplots(figsize=(8, 8))\n",
    "        ax_input.imshow(test_img[:, :, n_slice, 0], cmap='gray')\n",
    "        ax_input.set_title(\"Input Image (FLAIR)\")\n",
    "        ax_input.axis('off')  # Hide the axes\n",
    "\n",
    "        # Display the segmented image\n",
    "        fig_segmented, ax_segmented = plt.subplots(figsize=(8, 8))\n",
    "        ax_segmented.imshow(test_prediction_argmax[:, :, n_slice])\n",
    "        ax_segmented.set_title(\"Segmentation\")\n",
    "        ax_segmented.axis('off')  # Hide the axes\n",
    "\n",
    "        # Message based on the presence of the tumor\n",
    "        message = \"All good, no problem!\" if not tumor_found else \"Problem detected, a tumor has been identified!\"\n",
    "\n",
    "        # Return the figures and the message\n",
    "        return (fig_input, fig_segmented, message)\n",
    "\n",
    "    except Exception as e:\n",
    "        return None, None, f\"Error: {str(e)}\"\n",
    "\n",
    "def stop_processing():\n",
    "    \"\"\"Function to stop the processing.\"\"\"\n",
    "    global stop_flag\n",
    "    stop_flag.set()\n",
    "    return \"Processing stopped.\"\n",
    "\n",
    "# Gradio interface with a \"Stop Processing\" button\n",
    "iface = gr.Interface(\n",
    "    fn=interactive_segmentation,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload MRI Scan (.npy)\", file_types=[\".npy\"]),\n",
    "        gr.Button(\"Stop Processing\", variant=\"secondary\", elem_id=\"stop-btn\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Plot(label=\"Input Image (FLAIR)\"),\n",
    "        gr.Plot(label=\"Segmentation Result\"),\n",
    "        gr.Textbox(label=\"Diagnosis Message\")\n",
    "    ],\n",
    "    title=\"Brain Tumor Segmentation\",\n",
    "    description=\"Upload an MRI scan in .npy format (3 channels: FLAIR, T1ce, T2) to segment the brain tumor using a pre-trained 3D U-Net model from 'brats_3d.hdf5'.\",\n",
    "    live=False  # Set to False to avoid continuous execution\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2deec-2be2-4e80-8bf7-1c13f0cfc8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
